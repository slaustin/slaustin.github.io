---
title: 'The Mutual Information: Detecting and Evaluating Dependencies between Variables'
authors:
- R. Steuer
- J. Kurths
- C. O. Daub
- J. Weise
- J. Selbig
date: '2002-01-01'
publishDate: '2025-09-05T20:10:27.150361Z'
publication_types:
- article-journal
publication: '*Bioinformatics*'
doi: 10.1093/bioinformatics/18.suppl_2.S231
abstract: "Motivation: Clustering co-expressed genes usually requires the definition
  of 'distance' or 'similarity' between measured datasets, the most common choices
  being Pearson correlation or Euclidean distance. With the size of available datasets
  steadily increasing, it has become feasible to consider other, more general, definitions
  as well. One alternative, based on information theory, is the mutual information,
  providing a general measure of dependencies between variables. While the use of
  mutual information in cluster analysis and visualization of large-scale gene expression
  data has been suggested previously, the earlier studies did not focus on comparing
  different algorithms to estimate the mutual information from finite data. Results:
  Here we describe and review several approaches to estimate the mutual information
  from finite datasets. Our findings show that the algorithms used so far may be quite
  substantially improved upon. In particular when dealing with small datasets, finite
  sample effects and other sources of potentially misleading results have to be taken
  into account. oÌ§pyright Oxford University Press 2002."
---
