@article{Steuer2002,
 abstract = {Motivation: Clustering co-expressed genes usually requires the definition of 'distance' or 'similarity' between measured datasets, the most common choices being Pearson correlation or Euclidean distance. With the size of available datasets steadily increasing, it has become feasible to consider other, more general, definitions as well. One alternative, based on information theory, is the mutual information, providing a general measure of dependencies between variables. While the use of mutual information in cluster analysis and visualization of large-scale gene expression data has been suggested previously, the earlier studies did not focus on comparing different algorithms to estimate the mutual information from finite data. Results: Here we describe and review several approaches to estimate the mutual information from finite datasets. Our findings show that the algorithms used so far may be quite substantially improved upon. In particular when dealing with small datasets, finite sample effects and other sources of potentially misleading results have to be taken into account. oÌ§pyright Oxford University Press 2002.},
 author = {Steuer, R. and Kurths, J. and Daub, C. O. and Weise, J. and Selbig, J.},
 doi = {10.1093/bioinformatics/18.suppl_2.S231},
 issn = {13674803},
 journal = {Bioinformatics},
 number = {SUPPL. 2},
 pages = {231--240},
 pmid = {12386007},
 title = {The Mutual Information: Detecting and Evaluating Dependencies between Variables},
 volume = {18},
 year = {2002}
}
